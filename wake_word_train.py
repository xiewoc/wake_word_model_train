import numpy as np
import librosa
import sounddevice as sd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gradio as gr
import os
import matplotlib.pyplot as plt
import logging
import platform
import tempfile
import shutil
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
model_path = Path(__file__).parent / "wake_word_model.pth"

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device_index = 0
if platform.system() == "Darwin" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device(f"mps:{device_index}")
    logger.info(f"Using MPS device: {device}")
elif torch.cuda.is_available():
    device = torch.device(f"cuda:{device_index}")
    logger.info(f"Using CUDA device: {device}")
else:
    device = torch.device("cpu")
    logger.info("GPU acceleration not available, using CPU")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ä¿®æ­£ç‰¹å¾ç»´åº¦å¸¸é‡
FEATURE_DIM = 84  # 40 MFCC + 40 Delta + 4å…¶ä»–ç‰¹å¾ï¼ˆä¿®æ­£ï¼‰

# å®šä¹‰æ›´å¤æ‚çš„Siameseç½‘ç»œ
class SiameseNetwork(nn.Module):
    def __init__(self, input_size=FEATURE_DIM, hidden_size=256, embedding_size=128):
        super(SiameseNetwork, self).__init__()
        self.embedding_net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.BatchNorm1d(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size // 2, embedding_size),
            nn.BatchNorm1d(embedding_size),
            nn.Tanh()  # ä½¿ç”¨Tanhé™åˆ¶è¾“å‡ºèŒƒå›´
        )
        
    def forward(self, x):
        output = self.embedding_net(x)
        return output
    
    def get_embedding(self, x):
        return self.forward(x)

# å¯¹æ¯”æŸå¤±å‡½æ•°
class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive

# è‡ªå®šä¹‰æ•°æ®é›†
class WakeWordDataset(Dataset):
    def __init__(self, wake_word_files, background_files=None, sample_rate=16000, n_mfcc=40):
        self.sample_rate = sample_rate
        self.n_mfcc = n_mfcc
        self.wake_word_features = []
        
        # åŠ è½½å”¤é†’è¯æ ·æœ¬
        for file in wake_word_files:
            if os.path.exists(file):
                try:
                    audio, _ = librosa.load(file, sr=sample_rate)
                    # éŸ³é¢‘é¢„å¤„ç†ï¼šå½’ä¸€åŒ–
                    audio = self.preprocess_audio(audio)
                    feature = self.extract_features(audio)
                    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰æ•ˆä¸”ç»´åº¦æ­£ç¡®
                    if (not np.all(feature == 0) and 
                        not np.any(np.isnan(feature)) and 
                        len(feature) == FEATURE_DIM):
                        self.wake_word_features.append(feature)
                        logger.info(f"æˆåŠŸåŠ è½½å”¤é†’è¯æ ·æœ¬: {file}, ç‰¹å¾ç»´åº¦: {len(feature)}")
                    else:
                        logger.warning(f"æ— æ•ˆçš„å”¤é†’è¯æ ·æœ¬ç‰¹å¾: {file}, ç»´åº¦: {len(feature)}")
                except Exception as e:
                    logger.error(f"åŠ è½½å”¤é†’è¯æ ·æœ¬å¤±è´¥ {file}: {e}")
        
        # å¦‚æœå”¤é†’è¯æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨æ•°æ®å¢å¼º
        if len(self.wake_word_features) < 2:
            self.augment_wake_word_features()
        
        # åŠ è½½èƒŒæ™¯å™ªéŸ³æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬ï¼‰
        self.background_features = []
        if background_files:
            for file in background_files:
                if os.path.exists(file):
                    try:
                        audio, _ = librosa.load(file, sr=sample_rate)
                        audio = self.preprocess_audio(audio)
                        feature = self.extract_features(audio)
                        if (not np.all(feature == 0) and 
                            not np.any(np.isnan(feature)) and 
                            len(feature) == FEATURE_DIM):
                            self.background_features.append(feature)
                            logger.info(f"æˆåŠŸåŠ è½½èƒŒæ™¯æ ·æœ¬: {file}, ç‰¹å¾ç»´åº¦: {len(feature)}")
                        else:
                            logger.warning(f"æ— æ•ˆçš„èƒŒæ™¯æ ·æœ¬ç‰¹å¾: {file}, ç»´åº¦: {len(feature)}")
                    except Exception as e:
                        logger.error(f"åŠ è½½èƒŒæ™¯æ ·æœ¬å¤±è´¥ {file}: {e}")
        
        # å¦‚æœæ²¡æœ‰æä¾›èƒŒæ™¯å™ªéŸ³æˆ–æ•°é‡ä¸è¶³ï¼Œç”Ÿæˆä¸€äº›çœŸå®æ„Ÿçš„å™ªéŸ³ä½œä¸ºè´Ÿæ ·æœ¬
        if len(self.background_features) < 10:
            self.generate_background_features()
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾ç»´åº¦ä¸€è‡´
        self._validate_feature_dimensions()
        
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡ - å”¤é†’è¯æ ·æœ¬: {len(self.wake_word_features)}, èƒŒæ™¯æ ·æœ¬: {len(self.background_features)}")
    
    def preprocess_audio(self, audio):
        """éŸ³é¢‘é¢„å¤„ç†ï¼šå½’ä¸€åŒ–"""
        if len(audio) > 0:
            audio = audio / np.max(np.abs(audio) + 1e-8)  # é¿å…é™¤é›¶
        return audio
    
    def _validate_feature_dimensions(self):
        """éªŒè¯æ‰€æœ‰ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´"""
        for i, feat in enumerate(self.wake_word_features):
            if len(feat) != FEATURE_DIM:
                logger.warning(f"å”¤é†’è¯ç‰¹å¾ {i} ç»´åº¦ä¸æ­£ç¡®: {len(feat)}ï¼Œå°†è¿›è¡Œè£å‰ª/å¡«å……")
                self.wake_word_features[i] = self._fix_feature_dimension(feat)
        
        for i, feat in enumerate(self.background_features):
            if len(feat) != FEATURE_DIM:
                logger.warning(f"èƒŒæ™¯ç‰¹å¾ {i} ç»´åº¦ä¸æ­£ç¡®: {len(feat)}ï¼Œå°†è¿›è¡Œè£å‰ª/å¡«å……")
                self.background_features[i] = self._fix_feature_dimension(feat)
    
    def _fix_feature_dimension(self, feature):
        """ä¿®å¤ç‰¹å¾ç»´åº¦"""
        if len(feature) < FEATURE_DIM:
            # å¡«å……
            return np.pad(feature, (0, FEATURE_DIM - len(feature)), mode='constant')
        else:
            # è£å‰ª
            return feature[:FEATURE_DIM]
    
    def augment_wake_word_features(self):
        """æ”¹è¿›çš„æ•°æ®å¢å¼ºï¼šé€šè¿‡éŸ³é¢‘å˜æ¢ç”Ÿæˆæ›´å¤šæ ·æœ¬"""
        logger.info("æ­£åœ¨è¿›è¡Œæ•°æ®å¢å¼º...")
        original_features = self.wake_word_features.copy()
        
        for feature in original_features:
            # æ·»åŠ é«˜æ–¯å™ªå£°
            noise = np.random.normal(0, 0.02, feature.shape)
            augmented_feature = feature + noise
            self.wake_word_features.append(augmented_feature)
            
            # éšæœºç¼©æ”¾
            scale = np.random.uniform(0.8, 1.2)
            scaled_feature = feature * scale
            self.wake_word_features.append(scaled_feature)
            
            # æ—¶é—´åç§»
            shift = np.random.randint(-8, 8)
            shifted_feature = np.roll(feature, shift)
            self.wake_word_features.append(shifted_feature)
            
            # é¢‘ç‡åç§»ï¼ˆæ¨¡æ‹ŸéŸ³é«˜å˜åŒ–ï¼‰
            freq_shift = np.random.uniform(0.9, 1.1)
            freq_shifted_feature = feature * freq_shift
            self.wake_word_features.append(freq_shifted_feature)
        
        logger.info(f"æ•°æ®å¢å¼ºåï¼Œå”¤é†’è¯æ ·æœ¬æ•°é‡: {len(self.wake_word_features)}")
    
    def generate_background_features(self):
        """ç”Ÿæˆæ›´çœŸå®çš„èƒŒæ™¯å™ªéŸ³ç‰¹å¾"""
        logger.info("ç”Ÿæˆæ›´çœŸå®çš„èƒŒæ™¯å™ªéŸ³ç‰¹å¾...")
        num_background = max(30, len(self.wake_word_features) * 5)  # å¢åŠ è´Ÿæ ·æœ¬æ•°é‡
        
        # åŸºäºçœŸå®å”¤é†’è¯ç‰¹å¾ç”Ÿæˆè´Ÿæ ·æœ¬
        for wake_feature in self.wake_word_features:
            for _ in range(4):  # æ¯ä¸ªæ­£æ ·æœ¬ç”Ÿæˆ4ä¸ªè´Ÿæ ·æœ¬
                modified_feature = wake_feature.copy()
                
                # å¤šç§ä¿®æ”¹æ–¹å¼
                modification_type = np.random.choice(['shuffle', 'noise', 'scale', 'shift', 'invert'])
                
                if modification_type == 'shuffle':
                    np.random.shuffle(modified_feature)
                elif modification_type == 'noise':
                    noise = np.random.normal(0, 0.15, modified_feature.shape)
                    modified_feature += noise
                elif modification_type == 'scale':
                    scale = np.random.uniform(0.3, 2.5)
                    modified_feature *= scale
                elif modification_type == 'shift':
                    shift = np.random.randint(-15, 15)
                    modified_feature = np.roll(modified_feature, shift)
                elif modification_type == 'invert':
                    modified_feature = -modified_feature
                
                self.background_features.append(modified_feature)
        
        # æ·»åŠ å„ç§ç±»å‹çš„éšæœºå™ªéŸ³
        for _ in range(num_background):
            noise_type = np.random.choice(['realistic', 'white', 'pink', 'brown'])
            
            if noise_type == 'realistic':
                # æ¨¡æ‹ŸçœŸå®ç¯å¢ƒå™ªéŸ³
                base = np.random.randn(FEATURE_DIM) * 0.08
                speech_pattern = np.sin(np.linspace(0, 3*np.pi, FEATURE_DIM)) * 0.15
                noise = base + speech_pattern + np.random.randn(FEATURE_DIM) * 0.05
            elif noise_type == 'white':
                noise = np.random.randn(FEATURE_DIM) * 0.12
            elif noise_type == 'pink':
                # ç²‰çº¢å™ªéŸ³
                noise = np.random.randn(FEATURE_DIM) * np.linspace(1, 0.1, FEATURE_DIM)
            else:  # brown
                # å¸ƒæœ—å™ªéŸ³
                noise = np.cumsum(np.random.randn(FEATURE_DIM)) * 0.1
            
            self.background_features.append(noise)
        
        logger.info(f"ç”Ÿæˆçš„èƒŒæ™¯å™ªéŸ³ç‰¹å¾æ•°é‡: {len(self.background_features)}")
    
    def extract_features(self, audio_data):
        """æå–æ›´ä¸°å¯Œçš„éŸ³é¢‘ç‰¹å¾ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´"""
        if len(audio_data) < self.sample_rate:
            audio_data = np.pad(audio_data, (0, max(0, self.sample_rate - len(audio_data))), mode='constant')
        else:
            audio_data = audio_data[:self.sample_rate]
        
        # æå–å¤šç§ç‰¹å¾
        features = []
        
        try:
            # 1. MFCCç‰¹å¾
            mfccs = librosa.feature.mfcc(y=audio_data, sr=self.sample_rate, n_mfcc=self.n_mfcc)
            mfcc_mean = np.mean(mfccs.T, axis=0)
            features.extend(mfcc_mean)
            
            # 2. MFCCçš„ä¸€é˜¶å·®åˆ†
            mfcc_delta = librosa.feature.delta(mfccs)
            mfcc_delta_mean = np.mean(mfcc_delta.T, axis=0)
            features.extend(mfcc_delta_mean)
            
            # 3. é¢‘è°±è´¨å¿ƒ
            spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_centroid))
            
            # 4. è¿‡é›¶ç‡
            zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data)
            features.append(np.mean(zero_crossing_rate))
            
            # 5. é¢‘è°±å¸¦å®½
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_bandwidth))
            
            # 6. é¢‘è°±æ»šé™ç‚¹
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_rolloff))
            
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›é›¶ç‰¹å¾
            return np.zeros(FEATURE_DIM)
        
        feature = np.array(features)
        feature = np.nan_to_num(feature, nan=0.0, posinf=0.0, neginf=0.0)
        
        # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
        if len(feature) != FEATURE_DIM:
            logger.warning(f"ç‰¹å¾ç»´åº¦ä¸æ­£ç¡®: {len(feature)}ï¼ŒæœŸæœ›: {FEATURE_DIM}")
            feature = self._fix_feature_dimension(feature)
        
        return feature
    
    def __len__(self):
        return max(len(self.wake_word_features), 5) * 20  # å¢åŠ æ ·æœ¬æ•°é‡
    
    def __getitem__(self, idx):
        # æ”¹è¿›çš„æ­£è´Ÿæ ·æœ¬å¹³è¡¡ç”Ÿæˆ
        if len(self.wake_word_features) >= 2 and np.random.random() < 0.5:
            # æ­£æ ·æœ¬å¯¹ï¼šä¸¤ä¸ªå”¤é†’è¯æ ·æœ¬
            idx1, idx2 = np.random.choice(len(self.wake_word_features), 2, replace=False)
            feature1 = self.wake_word_features[idx1]
            feature2 = self.wake_word_features[idx2]
            label = 0  # ç›¸ä¼¼
        else:
            # è´Ÿæ ·æœ¬å¯¹
            if len(self.wake_word_features) > 0 and len(self.background_features) > 0:
                if np.random.random() < 0.7:
                    # å”¤é†’è¯ vs èƒŒæ™¯å™ªéŸ³ (70%)
                    idx1 = np.random.randint(len(self.wake_word_features))
                    idx2 = np.random.randint(len(self.background_features))
                    feature1 = self.wake_word_features[idx1]
                    feature2 = self.background_features[idx2]
                else:
                    # èƒŒæ™¯å™ªéŸ³ vs èƒŒæ™¯å™ªéŸ³ (30%)
                    idx1, idx2 = np.random.choice(len(self.background_features), 2, replace=False)
                    feature1 = self.background_features[idx1]
                    feature2 = self.background_features[idx2]
                label = 1  # ä¸ç›¸ä¼¼
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ
                feature1 = np.random.randn(FEATURE_DIM) * 0.1
                feature2 = np.random.randn(FEATURE_DIM) * 0.1
                label = 1
        
        return (torch.FloatTensor(feature1), 
                torch.FloatTensor(feature2), 
                torch.FloatTensor([label]))

# è¯­éŸ³å”¤é†’æ£€æµ‹å™¨
class AdvancedVoiceWakeWord:
    def __init__(self, sample_rate=16000, n_mfcc=40, threshold=0.7):
        self.sample_rate = sample_rate
        self.n_mfcc = n_mfcc
        self.threshold = threshold
        self.model = None
        self.wake_word_embeddings = []
        
    def train(self, wake_word_files, background_files=None, epochs=100, lr=0.0005):
        """è®­ç»ƒæ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å”¤é†’è¯æ ·æœ¬
        if not wake_word_files or len(wake_word_files) == 0:
            raise ValueError("æ²¡æœ‰æä¾›å”¤é†’è¯æ ·æœ¬æ–‡ä»¶")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        try:
            dataset = WakeWordDataset(wake_word_files, background_files, self.sample_rate, self.n_mfcc)
            
            if len(dataset.wake_word_features) < 2:
                raise ValueError("æœ‰æ•ˆçš„å”¤é†’è¯æ ·æœ¬ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºæ­£æ ·æœ¬å¯¹")
                
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80%è®­ç»ƒ, 20%éªŒè¯)
            train_size = int(0.8 * len(dataset))
            val_size = len(dataset) - train_size
            train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
            
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = SiameseNetwork(input_size=FEATURE_DIM).to(device)
        criterion = ContrastiveLoss(margin=0.8)
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨å‚æ•°
        optimizer = optim.Adam(
            self.model.parameters(), 
            lr=lr, 
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=epochs,
            eta_min=lr * 0.01
        )
        
        # æ¢¯åº¦è£å‰ª
        max_grad_norm = 1.0
        
        # è®­ç»ƒæ¨¡å‹
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        best_val_accuracy = 0
        patience_counter = 0
        patience = 25  # å¢åŠ æ—©åœè€å¿ƒå€¼
        
        self.model.train()
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_train_loss = 0
            train_batch_count = 0
            train_correct = 0
            train_total = 0
            
            for feature1, feature2, label in train_loader:
                if feature1.shape[0] != feature2.shape[0]:
                    continue
                    
                feature1, feature2, label = feature1.to(device), feature2.to(device), label.to(device)
                
                optimizer.zero_grad()
                output1 = self.model(feature1)
                output2 = self.model(feature2)
                
                # è®¡ç®—å‡†ç¡®ç‡
                with torch.no_grad():
                    distances = nn.functional.pairwise_distance(output1, output2)
                    predictions = (distances > 0.5).float()
                    train_correct += (predictions == label.squeeze()).sum().item()
                    train_total += label.size(0)
                
                loss = criterion(output1, output2, label)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                
                optimizer.step()
                
                epoch_train_loss += loss.item()
                train_batch_count += 1
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            epoch_val_loss = 0
            val_batch_count = 0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for feature1, feature2, label in val_loader:
                    if feature1.shape[0] != feature2.shape[0]:
                        continue
                        
                    feature1, feature2, label = feature1.to(device), feature2.to(device), label.to(device)
                    
                    output1 = self.model(feature1)
                    output2 = self.model(feature2)
                    
                    loss = criterion(output1, output2, label)
                    epoch_val_loss += loss.item()
                    val_batch_count += 1
                    
                    distances = nn.functional.pairwise_distance(output1, output2)
                    predictions = (distances > 0.5).float()
                    val_correct += (predictions == label.squeeze()).sum().item()
                    val_total += label.size(0)
            
            if train_batch_count > 0 and val_batch_count > 0:
                avg_train_loss = epoch_train_loss / train_batch_count
                avg_val_loss = epoch_val_loss / val_batch_count
                train_accuracy = train_correct / train_total if train_total > 0 else 0
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                train_losses.append(avg_train_loss)
                val_losses.append(avg_val_loss)
                train_accuracies.append(train_accuracy)
                val_accuracies.append(val_accuracy)
                
                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()
                
                # æ—©åœæœºåˆ¶
                if val_accuracy > best_val_accuracy:
                    best_val_accuracy = val_accuracy
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_state = self.model.state_dict().copy()
                    logger.info(f"æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.4f}")
                else:
                    patience_counter += 1
                
                if epoch % 5 == 0 or epoch == epochs - 1:
                    current_lr = optimizer.param_groups[0]['lr']
                    logger.info(f'Epoch [{epoch}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, '
                               f'Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, LR: {current_lr:.6f}')
                
                # æ—©åœæ£€æŸ¥
                if patience_counter >= patience:
                    logger.info(f"æ—©åœè§¦å‘ï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.4f}")
                    # æ¢å¤æœ€ä½³æ¨¡å‹
                    self.model.load_state_dict(best_model_state)
                    break
            else:
                train_losses.append(0)
                val_losses.append(0)
                train_accuracies.append(0)
                val_accuracies.append(0)
        
        # è®¡ç®—å”¤é†’è¯çš„å‚è€ƒåµŒå…¥
        self.model.eval()
        self.wake_word_embeddings = []
        for feature in dataset.wake_word_features:
            with torch.no_grad():
                embedding = self.model(torch.FloatTensor(feature).unsqueeze(0).to(device))
                self.wake_word_embeddings.append(embedding.cpu().numpy())
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_metrics(train_losses, val_losses, train_accuracies, val_accuracies)
        
        # åˆ†æè®­ç»ƒæ›²çº¿
        curve_analysis = self.analyze_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)
        logger.info(curve_analysis)
        
        return train_losses, curve_analysis
    
    def analyze_training_curves(self, train_losses, val_losses, train_accuracies, val_accuracies):
        """åˆ†æè®­ç»ƒæ›²çº¿"""
        if len(train_losses) < 2:
            return "è®­ç»ƒè½®æ•°ä¸è¶³ï¼Œæ— æ³•åˆ†ææ›²çº¿"
        
        analysis = "è®­ç»ƒæ›²çº¿åˆ†æ:\n"
        analysis += "=" * 50 + "\n"
        
        final_train_loss = train_losses[-1]
        final_val_loss = val_losses[-1] if val_losses else 0
        final_train_acc = train_accuracies[-1] if train_accuracies else 0
        final_val_acc = val_accuracies[-1] if val_accuracies else 0
        
        analysis += f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.4f}\n"
        analysis += f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.4f}\n"
        analysis += f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.4f}\n"
        analysis += f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.4f}\n"
        
        # è¿‡æ‹Ÿåˆæ£€æŸ¥
        if val_losses and len(val_losses) > 10:
            if val_losses[-1] > val_losses[-10] and train_losses[-1] < train_losses[-10]:
                analysis += "âš ï¸  æ£€æµ‹åˆ°å¯èƒ½è¿‡æ‹Ÿåˆ: è®­ç»ƒæŸå¤±ä¸‹é™ä½†éªŒè¯æŸå¤±ä¸Šå‡\n"
        
        analysis += "\nè¯Šæ–­å»ºè®®:\n"
        
        if final_train_loss > 0.5:
            analysis += "âŒ è®­ç»ƒæŸå¤±è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡\n"
        elif final_train_loss > 0.2:
            analysis += "âš ï¸  è®­ç»ƒæŸå¤±é€‚ä¸­ï¼Œå¯ä»¥ç»§ç»­ä¼˜åŒ–\n"
        else:
            analysis += "âœ… è®­ç»ƒæŸå¤±å¾ˆä½ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½\n"
        
        if final_val_acc < 0.7:
            analysis += "âŒ éªŒè¯å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ ·æœ¬å¤šæ ·æ€§æˆ–è°ƒæ•´æ¨¡å‹\n"
        elif final_val_acc < 0.85:
            analysis += "âš ï¸  éªŒè¯å‡†ç¡®ç‡é€‚ä¸­ï¼Œè¿˜æœ‰æå‡ç©ºé—´\n"
        else:
            analysis += "âœ… éªŒè¯å‡†ç¡®ç‡å¾ˆé«˜ï¼Œè®­ç»ƒæˆåŠŸ\n"
        
        # è®­ç»ƒéªŒè¯å·®è·åˆ†æ
        if train_accuracies and val_accuracies:
            gap = train_accuracies[-1] - val_accuracies[-1]
            if gap > 0.15:
                analysis += "âš ï¸  è®­ç»ƒéªŒè¯å·®è·è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ\n"
        
        return analysis
    
    def plot_training_metrics(self, train_losses, val_losses, train_accuracies, val_accuracies):
        """ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡"""
        plt.figure(figsize=(15, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Training Loss')
        if val_losses:
            plt.plot(val_losses, label='Validation Loss')
        plt.title('Loss Curves')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        if train_accuracies:
            plt.plot(train_accuracies, label='Training Accuracy')
        if val_accuracies:
            plt.plot(val_accuracies, label='Validation Accuracy')
        plt.title('Accuracy Curves')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def is_wake_word(self, audio_data):
        """æ£€æµ‹æ˜¯å¦ä¸ºå”¤é†’è¯"""
        if self.model is None or not self.wake_word_embeddings:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return False, 0.0
        
        # éŸ³é¢‘é¢„å¤„ç†
        audio_data = self.preprocess_audio(audio_data)
        
        # æå–ç‰¹å¾
        feature = self.extract_features(audio_data)
        
        # è°ƒè¯•ä¿¡æ¯
        feature_mean = np.mean(feature)
        feature_std = np.std(feature)
        print(f"ç‰¹å¾æå–ç»“æœ: å‡å€¼={feature_mean:.4f}, æ ‡å‡†å·®={feature_std:.4f}")
        
        # è·å–åµŒå…¥å‘é‡
        self.model.eval()
        with torch.no_grad():
            input_tensor = torch.FloatTensor(feature).unsqueeze(0).to(device)
            embedding = self.model(input_tensor).cpu().numpy()
        
        embedding_mean = np.mean(embedding)
        embedding_std = np.std(embedding)
        print(f"åµŒå…¥å‘é‡: å‡å€¼={embedding_mean:.4f}, æ ‡å‡†å·®={embedding_std:.4f}")
        
        # è®¡ç®—æ¬§æ°è·ç¦»ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        distances = []
        for ref_embedding in self.wake_word_embeddings:
            distance = np.linalg.norm(embedding - ref_embedding)
            distances.append(distance)
        
        min_distance = min(distances) if distances else float('inf')
        avg_distance = np.mean(distances) if distances else float('inf')
        
        # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        similarity = 1.0 / (1.0 + min_distance)
        avg_similarity = 1.0 / (1.0 + avg_distance) if avg_distance != float('inf') else 0
        
        print(f"æœ€å°è·ç¦»: {min_distance:.4f}, å¹³å‡è·ç¦»: {avg_distance:.4f}")
        print(f"æœ€å¤§ç›¸ä¼¼åº¦: {similarity:.4f}, å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
        
        # æ›´ä¸¥æ ¼çš„æ£€æµ‹æ¡ä»¶
        is_wake = (
            similarity > self.threshold and
            avg_similarity > self.threshold * 0.5 and
            min_distance < 0.8  # è·ç¦»é˜ˆå€¼
        )
        
        return is_wake, similarity
    
    def preprocess_audio(self, audio_data):
        """éŸ³é¢‘é¢„å¤„ç†ï¼šå½’ä¸€åŒ–"""
        if len(audio_data) > 0:
            max_val = np.max(np.abs(audio_data))
            if max_val > 1e-8:  # é¿å…é™¤é›¶
                audio_data = audio_data / max_val
        return audio_data
    
    def extract_features(self, audio_data):
        """æå–ç‰¹å¾"""
        if len(audio_data) < self.sample_rate:
            audio_data = np.pad(audio_data, (0, max(0, self.sample_rate - len(audio_data))), mode='constant')
        else:
            audio_data = audio_data[:self.sample_rate]
        
        features = []
        
        try:
            mfccs = librosa.feature.mfcc(y=audio_data, sr=self.sample_rate, n_mfcc=self.n_mfcc)
            mfcc_mean = np.mean(mfccs.T, axis=0)
            features.extend(mfcc_mean)
            
            mfcc_delta = librosa.feature.delta(mfccs)
            mfcc_delta_mean = np.mean(mfcc_delta.T, axis=0)
            features.extend(mfcc_delta_mean)
            
            spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_centroid))
            
            zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data)
            features.append(np.mean(zero_crossing_rate))
            
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_bandwidth))
            
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=self.sample_rate)
            features.append(np.mean(spectral_rolloff))
            
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(FEATURE_DIM)
        
        feature = np.array(features)
        feature = np.nan_to_num(feature, nan=0.0, posinf=0.0, neginf=0.0)
        
        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if len(feature) != FEATURE_DIM:
            feature = np.pad(feature, (0, max(0, FEATURE_DIM - len(feature))), mode='constant')[:FEATURE_DIM]
        
        return feature

# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
def process_uploaded_files(files, target_dir):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    
    saved_paths = []
    for file in files:
        if hasattr(file, 'name'):
            src_path = file.name
        else:
            src_path = file
            
        filename = os.path.basename(src_path)
        dest_path = os.path.join(target_dir, filename)
        shutil.copy2(src_path, dest_path)
        saved_paths.append(dest_path)
    
    return saved_paths

# Gradio UIå‡½æ•°
def train_model_ui(wake_word_files, background_files, epochs, lr, threshold):
    """Gradioè®­ç»ƒç•Œé¢"""
    try:
        if not wake_word_files:
            return "è¯·å…ˆä¸Šä¼ å”¤é†’è¯æ ·æœ¬", None, ""
        
        with tempfile.TemporaryDirectory() as temp_dir:
            wake_word_dir = os.path.join(temp_dir, "wake_word")
            background_dir = os.path.join(temp_dir, "background")
            
            wake_word_paths = process_uploaded_files(wake_word_files, wake_word_dir)
            background_paths = process_uploaded_files(background_files, background_dir) if background_files else None
            
            detector = AdvancedVoiceWakeWord(threshold=threshold)
            losses, curve_analysis = detector.train(wake_word_paths, background_paths, epochs=int(epochs), lr=lr)
            
            if detector.model is not None:
                torch.save({
                    'model_state_dict': detector.model.state_dict(),
                    'wake_word_embeddings': detector.wake_word_embeddings,
                    'threshold': threshold,
                    'input_size': FEATURE_DIM
                }, str(model_path))
                logger.info("æ¨¡å‹å·²ä¿å­˜")
            
            return "è®­ç»ƒå®Œæˆï¼", 'training_metrics.png', curve_analysis
            
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return f"è®­ç»ƒå¤±è´¥: {str(e)}", None, ""

def detect_wake_word_ui(audio_file, threshold):
    """Gradioæ£€æµ‹ç•Œé¢"""
    try:
        if audio_file is None:
            return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", 0.0
        
        if hasattr(audio_file, 'name'):
            audio_path = audio_file.name
        else:
            audio_path = audio_file
        
        detector = AdvancedVoiceWakeWord(threshold=threshold)
        
        # æ”¹è¿›çš„æ¨¡å‹åŠ è½½
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            except:
                # å…¼å®¹æ—§ç‰ˆæœ¬
                checkpoint = torch.load(model_path, map_location=device)
            
            input_size = checkpoint.get('input_size', FEATURE_DIM)
            detector.model = SiameseNetwork(input_size=input_size).to(device)
            detector.model.load_state_dict(checkpoint['model_state_dict'])
            detector.wake_word_embeddings = checkpoint['wake_word_embeddings']
            detector.model.eval()
        else:
            return "è¯·å…ˆè®­ç»ƒæ¨¡å‹", 0.0
        
        audio, sr = librosa.load(audio_path, sr=16000)
        is_wake, confidence = detector.is_wake_word(audio)
        
        result_text = "âœ… æ£€æµ‹åˆ°å”¤é†’è¯ï¼" if is_wake else "âŒ æœªæ£€æµ‹åˆ°å”¤é†’è¯"
        confidence_text = f"ç½®ä¿¡åº¦: {confidence:.4f}"
        
        return f"{result_text}\n{confidence_text}", confidence
        
    except Exception as e:
        logger.error(f"æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return f"æ£€æµ‹å¤±è´¥: {str(e)}", 0.0

def test_model_ui(wake_word_files, test_files, threshold):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    try:
        if not wake_word_files or not test_files:
            return "è¯·å…ˆä¸Šä¼ å”¤é†’è¯æ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬"
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            wake_word_dir = os.path.join(temp_dir, "wake_word")
            test_dir = os.path.join(temp_dir, "test")
            
            # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
            wake_word_paths = process_uploaded_files(wake_word_files, wake_word_dir)
            test_paths = process_uploaded_files(test_files, test_dir)
            
            # åŠ è½½æ¨¡å‹
            detector = AdvancedVoiceWakeWord(threshold=threshold)
            if os.path.exists(model_path):
                try:
                    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
                except:
                    checkpoint = torch.load(model_path, map_location=device)
                
                input_size = checkpoint.get('input_size', FEATURE_DIM)
                detector.model = SiameseNetwork(input_size=input_size).to(device)
                detector.model.load_state_dict(checkpoint['model_state_dict'])
                detector.wake_word_embeddings = checkpoint['wake_word_embeddings']
                detector.model.eval()
            else:
                return "è¯·å…ˆè®­ç»ƒæ¨¡å‹"
            
            # æµ‹è¯•æ€§èƒ½
            results = []
            confidences = []
            for test_file in test_paths:
                audio, sr = librosa.load(test_file, sr=16000)
                is_wake, confidence = detector.is_wake_word(audio)
                results.append(is_wake)
                confidences.append(confidence)
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            total = len(results)
            correct = sum(results)
            accuracy = correct / total if total > 0 else 0
            avg_confidence = np.mean(confidences) if confidences else 0
            
            report = f"ğŸ“Š æµ‹è¯•ç»“æœ:\n"
            report += f"æ€»æ ·æœ¬æ•°: {total}\n"
            report += f"æ­£ç¡®è¯†åˆ«: {correct}\n"
            report += f"å‡†ç¡®ç‡: {accuracy:.4f}\n"
            report += f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f}\n\n"
            report += f"è¯¦ç»†ç»“æœ:\n"
            
            for i, (test_file, is_wake, confidence) in enumerate(zip(test_paths, results, confidences)):
                status = "âœ“" if is_wake else "âœ—"
                report += f"{i+1}. {os.path.basename(test_file)}: {status} (ç½®ä¿¡åº¦: {confidence:.4f})\n"
            
            return report
            
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return f"æµ‹è¯•å¤±è´¥: {str(e)}"

# åˆ›å»ºGradioç•Œé¢
def create_gradio_ui():
    with gr.Blocks(title="æ”¹è¿›ç‰ˆå°æ ·æœ¬è¯­éŸ³å”¤é†’è®­ç»ƒç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ¯ æ”¹è¿›ç‰ˆå°æ ·æœ¬è¯­éŸ³å”¤é†’è®­ç»ƒç³»ç»Ÿ")
        
        with gr.Tab("è®­ç»ƒæ¨¡å‹"):
            gr.Markdown("## è®­ç»ƒè¯­éŸ³å”¤é†’æ¨¡å‹")
            with gr.Row():
                with gr.Column():
                    wake_word_files = gr.File(file_count="multiple", label="ä¸Šä¼ å”¤é†’è¯æ ·æœ¬(WAVæ ¼å¼)", file_types=[".wav"])
                    background_files = gr.File(file_count="multiple", label="ä¸Šä¼ èƒŒæ™¯å™ªéŸ³æ ·æœ¬(WAVæ ¼å¼ï¼Œå¯é€‰)", file_types=[".wav"])
                    epochs = gr.Slider(50, 300, value=120, step=10, label="è®­ç»ƒè½®æ•°")
                    lr = gr.Slider(0.0001, 0.01, value=0.0005, step=0.0001, label="å­¦ä¹ ç‡")
                    threshold = gr.Slider(0.5, 0.95, value=0.75, step=0.05, label="æ£€æµ‹é˜ˆå€¼")
                    train_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary")
                
                with gr.Column():
                    output_text = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", lines=3)
                    output_plot = gr.Image(label="è®­ç»ƒæŒ‡æ ‡å›¾è¡¨")
                    analysis_output = gr.Textbox(label="æ›²çº¿åˆ†æ", lines=10)
            
            train_btn.click(
                fn=train_model_ui,
                inputs=[wake_word_files, background_files, epochs, lr, threshold],
                outputs=[output_text, output_plot, analysis_output]
            )
        
        with gr.Tab("æ£€æµ‹å”¤é†’è¯"):
            gr.Markdown("## æ£€æµ‹éŸ³é¢‘ä¸­çš„å”¤é†’è¯")
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶(WAVæ ¼å¼)", type="filepath")
                    detect_threshold = gr.Slider(0.5, 0.95, value=0.75, step=0.05, label="æ£€æµ‹é˜ˆå€¼")
                    detect_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary")
                
                with gr.Column():
                    detect_output = gr.Textbox(label="æ£€æµ‹ç»“æœ", lines=3)
                    confidence_gauge = gr.Number(label="ç½®ä¿¡åº¦", interactive=False)
            
            detect_btn.click(
                fn=detect_wake_word_ui,
                inputs=[audio_input, detect_threshold],
                outputs=[detect_output, confidence_gauge]
            )
        
        with gr.Tab("æµ‹è¯•æ¨¡å‹"):
            gr.Markdown("## æµ‹è¯•æ¨¡å‹æ€§èƒ½")
            with gr.Row():
                with gr.Column():
                    test_wake_word_files = gr.File(file_count="multiple", label="ä¸Šä¼ å”¤é†’è¯æ ·æœ¬", file_types=[".wav"])
                    test_files = gr.File(file_count="multiple", label="ä¸Šä¼ æµ‹è¯•æ ·æœ¬", file_types=[".wav"])
                    test_threshold = gr.Slider(0.5, 0.95, value=0.75, step=0.05, label="æ£€æµ‹é˜ˆå€¼")
                    test_btn = gr.Button("ğŸ§ª å¼€å§‹æµ‹è¯•", variant="primary")
                
                with gr.Column():
                    test_output = gr.Textbox(label="æµ‹è¯•ç»“æœ", lines=15)
            
            test_btn.click(
                fn=test_model_ui,
                inputs=[test_wake_word_files, test_files, test_threshold],
                outputs=[test_output]
            )
        
        with gr.Tab("ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ğŸ“– ä½¿ç”¨è¯´æ˜
            
            ### 1. è®­ç»ƒæ¨¡å‹
            - ä¸Šä¼ è‡³å°‘2-5ä¸ªæ¸…æ™°çš„å”¤é†’è¯æ ·æœ¬
            - å¯é€‰ä¸Šä¼ èƒŒæ™¯å™ªéŸ³æ ·æœ¬æé«˜æ¨¡å‹é²æ£’æ€§
            - å»ºè®®å‚æ•°: è®­ç»ƒè½®æ•°120-150, å­¦ä¹ ç‡0.0005, é˜ˆå€¼0.75
            
            ### 2. æ£€æµ‹å”¤é†’è¯
            - å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ
            - ä¸Šä¼ å¾…æ£€æµ‹çš„éŸ³é¢‘æ–‡ä»¶
            - è°ƒæ•´é˜ˆå€¼æ§åˆ¶æ£€æµ‹çµæ•åº¦
            
            ### 3. æµ‹è¯•æ¨¡å‹
            - ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½
            - æŸ¥çœ‹å‡†ç¡®ç‡å’Œç½®ä¿¡åº¦æŒ‡æ ‡
            
            ### ğŸ’¡ æœ€ä½³å®è·µ
            - å”¤é†’è¯æ ·æœ¬æ—¶é•¿1-2ç§’ï¼Œå‘éŸ³æ¸…æ™°ä¸€è‡´
            - èƒŒæ™¯æ ·æœ¬åŒ…å«å„ç§å™ªéŸ³å’Œè¯­éŸ³ç‰‡æ®µ
            - è®­ç»ƒå®Œæˆåä½¿ç”¨æµ‹è¯•é›†éªŒè¯æ•ˆæœ
            
            ### ğŸ¯ æ”¹è¿›ç‰¹æ€§
            - ä¿®æ­£äº†ç‰¹å¾ç»´åº¦é—®é¢˜
            - æ”¹è¿›äº†æ•°æ®å¢å¼ºå’Œè´Ÿæ ·æœ¬ç”Ÿæˆ
            - æ·»åŠ äº†éªŒè¯é›†å’Œæ—©åœæœºåˆ¶
            - ç»Ÿä¸€äº†è·ç¦»åº¦é‡æ ‡å‡†
            - å¢å¼ºäº†è°ƒè¯•ä¿¡æ¯
            """)
    
    return demo

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºGradioç•Œé¢å¹¶å¯åŠ¨
    demo = create_gradio_ui()
    
    # é…ç½®é˜Ÿåˆ—å‚æ•°
    demo.queue(
        status_update_rate="auto",
        api_open=False,
        max_size=10
    )
    
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7860, 
        share=False,
        show_error=True
    )

if __name__ == "__main__":
    main()